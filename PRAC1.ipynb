{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f8a5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUTHOR: Àlex López Diaz\n",
    "## Date: 20/11/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd81650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessaris\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# URLs a UTILITZAR\n",
    "URL_SCRAP = 'https://marvel.fandom.com/wiki/Character_Index'\n",
    "URL_SCRAP_A = 'https://marvel.fandom.com/wiki/Character_Index/B'\n",
    "CSV_PATH = './Result/'\n",
    "CSV_NAME = 'result_web_scrap_marvel.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374fb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##                    CAS BASE                       ##\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6a2c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAS BASE\n",
    "# Utilitzarem el cas base d'extraure l'informació d'una pàgina en contret\n",
    "# per automatitzar-ho més endavant\n",
    "\n",
    "page = requests.get(URL_SCRAP_A)\n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acfa9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraiem els fragments que contenen la informació dels personatges\n",
    "characters_soup = soup.find_all(\"span\",{\"class\": \"mw-headline\"})\n",
    "#print(characters_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b8fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funció que  extreu els noms dels personatges\n",
    "# Al estar la info del personatge i el nom en grups separats de l'html,\n",
    "# farem dos datasets amb la mateixa llargada i farem un \"merge\"\n",
    "def extract_characters_names(soup):\n",
    "    characters_soup = soup.find_all(\"span\",{\"class\": \"mw-headline\"})\n",
    "    charecters_names = []\n",
    "    for character in characters_soup:\n",
    "        if not character.string.startswith(\"Character Index\",0,15) and character.string != 'References':\n",
    "            charecters_names.append(character.string)\n",
    "    return charecters_names\n",
    "\n",
    "characters_names = extract_characters_names(soup)\n",
    "#print(characters_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c14136c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor de cada linia\n",
    "# Passem el nom per paràmetre per afegir-lo al registre\n",
    "def extract_character_info(name,tr):\n",
    "    # Hi ha registres sense foto\n",
    "    if tr.find(\"a\", {\"class\": \"image\"}) is None:\n",
    "        image = ''\n",
    "    else:\n",
    "        image = tr.find(\"a\", {\"class\": \"image\"})[\"href\"]\n",
    "    # Hi ha registres sense appearences\n",
    "    if tr.find(\"a\", title = True) is None:\n",
    "        appearence = ''\n",
    "    else:\n",
    "        appearence = tr.find(\"a\", title = True)[\"title\"]\n",
    "    # Hi ha registres sense descripcio\n",
    "    if tr.find(\"p\") is None:\n",
    "        description = ''\n",
    "    else:\n",
    "        description = tr.find(\"p\").text.strip()\n",
    "        \n",
    "    character_data = {\n",
    "        'name': name,\n",
    "        'image': image,\n",
    "        'appearences': appearence,\n",
    "        'description': description,\n",
    "    }\n",
    "    return character_data\n",
    "\n",
    "# Constructor de les linies de cada bloc\n",
    "# Itera tots els blocs d'informació per crear un registre per cada un\n",
    "def exctract_all_character_info(name, soup):\n",
    "    character_data = []\n",
    "    for i in soup.select(\"tr\"):\n",
    "        character_data.append(extract_character_info(name, i))\n",
    "    \n",
    "    return character_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b56916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prova per comprovar que les funcions funcionen\n",
    "character_data_soup = soup.find_all(\"table\",{\"class\": \"article-table\"})\n",
    "#print(exctract_all_character_info(characters_names[0],character_data_soup[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3759139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmem que els dos datasets, data i names tenen la mateixa llargada\n",
    "#print(len(character_data_soup))\n",
    "#print(len(characters_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7a281ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Funció que extreu tota la informació de la pàgina web\n",
    "# Crida a la funció exctract_all_character_info que crea registres per \n",
    "# cada article. Necessita un nom per afegir-lo.\n",
    "def scrap_web_with_info(soup):\n",
    "    character_data_soup = soup.find_all(\"table\",{\"class\": \"article-table\"})\n",
    "    characters_names = extract_characters_names(soup)\n",
    "    result = []\n",
    "    for i in range(0,len(character_data_soup)-1):\n",
    "        result = result + exctract_all_character_info(characters_names[i], character_data_soup[i])\n",
    "    return result\n",
    "\n",
    "#print(len(scrap_web_with_info(soup)))\n",
    "\n",
    "# Cridant aquesta funció podem realitzar l'extracció del cas base.\n",
    "result = scrap_web_with_info(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b945a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funció per exportar el dataset a un csv\n",
    "def export_data(result):\n",
    "    header = result[0].keys()\n",
    "    dir_path = os.getcwd()\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(CSV_PATH + CSV_NAME, sep=\";\", header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ae64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "##             PROCEDIMENT PRINCIPAL                 ##\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b67e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenim les dades de la web índex\n",
    "page = requests.get(URL_SCRAP)\n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "583bfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem el nom dels index a afegir a la url base\n",
    "# Cada web que abans hem anomenat \"cas base\" té per URLla combinació\n",
    "# de la URL base + l'identificador de l'índex\n",
    "sufixes_to_add = []\n",
    "\n",
    "for i in soup.find_all(\"span\",{\"class\": \"mw-headline\"}):\n",
    "    if i.select(\"a\"):\n",
    "        sufixes_to_add.append(i[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cb77b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sufixes_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc13434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping page: https://marvel.fandom.com/wiki/Character_Index/A\n",
      "Sleep for: 2 seconds.\n",
      "Scrapping page: https://marvel.fandom.com/wiki/Character_Index/B\n",
      "Sleep for: 2 seconds.\n",
      "Scrapping page: https://marvel.fandom.com/wiki/Character_Index/C\n",
      "Sleep for: 3 seconds.\n",
      "TOT OK\n"
     ]
    }
   ],
   "source": [
    "#SCRAP!!!!!!!\n",
    "\n",
    "result_final = []\n",
    "# Capem per no iterar totes les pàgines\n",
    "for i in range(0,3): # Canviar el 3 per 26 si es vol l'execució completa\n",
    "    # Afegim el valor recollit a la url base\n",
    "    url = URL_SCRAP + '/' + sufixes_to_add[i]\n",
    "    print(\"Scrapping page: \" + URL_SCRAP + '/' + sufixes_to_add[i])\n",
    "    \n",
    "    i_page = requests.get(url)\n",
    "    i_soup = BeautifulSoup(i_page.content)\n",
    "    \n",
    "    # Extraiem la informació cridant a la funció scrap_web_with_info\n",
    "    result_final = result_final + scrap_web_with_info(i_soup)\n",
    "    \n",
    "    # Realitzem parades random entre 1 i 5 segons per evitar bloquejos\n",
    "    aux = randint(1,5)\n",
    "    print(f\"Sleep for: {aux} seconds.\")\n",
    "    sleep(aux)\n",
    "    \n",
    "export_data(result_final)\n",
    "print(\"TOT OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b4b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
